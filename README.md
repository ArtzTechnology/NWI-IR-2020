# NWI-IR-2020

## Introduction 
This project is created in response to Chris Buckelys email [thread](https://groups.google.com/g/trec-covid/c/DY9e0rZmIDA/m/6LWFpz9QBwAJ?pli=1) regarding the [TREC-COVID](https://ir.nist.gov/covidSubmit/index.html) challenge. He claims that a bare-bones bag of words information retrieval model would outperform some modern sophisticated models on the [CORD-19](https://www.semanticscholar.org/cord19) dataset. In this project we mimicked his approach and tried to improve it on this modern biomedical datatset. Each script contains a short description of its purpose and which folder or files are used as input and output. Please edit the .gitignore file to pull the folders you need for a specific script and checkout the Makefile for compiling options. In this respository we excluded the original dataset and all intermediate results due to upload issues. Below is a description of the general program flow. 

## Preprocess
We only used the 62.736 documents of the CORD-19 dataset of 16th July 2020 that were included as parsed Pubmed Central json files. We exlcuded the other files in the metadata.csv and judgements.txt with the script filter.cpp. We then created two bag of words for each document, one for the document and one for its metadata, with the script [extract_text.py](extract_text.py). With the script [clean_files.cpp](clean_files.cpp) we filtered out all non alphabetic symbols and stopwords specified by the [Natural Language Toolkit](https://www.nltk.org/). Then we stemmed all words in the script [stem.py](stem.py) with the [Oleander Stemming Library](https://github.com/OleanderSoftware/OleanderStemmingLibrary). 

## Indexing
We started the indexing by counting the term frequencies per bag with script [term_frequencies.cpp](term_frequencies.cpp). Then we counted the document frequencies with the script [document_frequencies.cpp](document_frequencies.cpp). Next we calculated the inverse document frequencies with the script [inverse_document_frequencies.cpp](inverse_document_frequencies.cpp) with idf(w) = log( m + 1 / df(w)) where M is the number of documents and df is the document frequency of term w. We used another script called [transform_term_frequencies.cpp](transform_term_frequencies.cpp) to transform the term frequencies as tf(w) = ln(1+ln(1+tf(w)). Then we combined the transformed tf and idf values in the script [tf_idf_weights.cpp](tf_idf_weights.cpp). The final weights were calculated with pivoted document length normalization in the script [indexing.cpp](indexing.cpp), with the normalization given by 1 - b + b * document_length / average_document_length. We tried different values for b and 0.25 seemed to be the best. 

## Querying
To save some computing time we preprocessed the queries. With [extract_topics.py](extract_topics.py) we extracted the bag of words of each topic. As the other bags all non alphabetic symbols and English stopwords were removed and then all words were stemmed. With [query_frequencies.cpp](query_frequencies.cpp) we calculated the topics term frequencies. With the script [scores.cpp](scores.cpp) we calculate the score for each metadata and document bag, per file per topic. Then we used the script [tune.cpp](tune.cpp) to find the best average weight for the metadata and document score, which is 39% and 61% respectively. With the script [query.cpp](query.cpp) we created a list with the total ranking of each topic and saved it in unfiltered.txt. Then in the script [similarity.cpp](similarity.cpp) we retrieve a list per topic of all documents that are similar to the top 20 retrieved documents. Finally in the script [bonus.cpp](bonus.cpp) these documents receive a bonus factor. The results, which are retrieved by the trec_eval tool, can be found in our small paper.
